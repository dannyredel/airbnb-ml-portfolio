[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Amsterdam Airbnb Price Prediction: A Machine Learning Approach",
    "section": "",
    "text": "Inside-AirBnB Dataset\nFrom the project website: http://insideairbnb.com/about/\nInside Airbnb is a mission driven project that provides data and advocacy about Airbnb’s impact on residential communities.\nThe dataset contains 7000 Amsterdam listings (each with 75 features) and 300k textual reviews from airbnb.com, scraped in March 2023. Oldest listings have reviews from as far as 2009. Additionally, for each listing, the dataset includes the price of stay for each day (night) since March 2023 till March 2024, as well as whether this day is available for booking, also scraped in March 2023.\nThe features descriptions can be found in this sheet from Inside Airbnb.\nFile descriptions:\n\nlistings.csv: Detailed Listings data\ncalendar.csv: Detailed Calendar Data\nreviews.csv: Detailed Review Data\nlistings.csv: Summary information and metrics for listings in Amsterdam (good for visualisations).\nreviews.csv: Summary Review data and Listing ID (to facilitate time based analytics and visualisations linked to a listing)\nneighbourhoods.csv Neighbourhood list for geo filter. Sourced from city or open source GIS files\nneighbourhoods.geojson GeoJSON file of neighbourhoods of the city.",
    "crumbs": [
      "Intro: Airbnb ML Project"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "airbnb_dataset_info.html",
    "href": "airbnb_dataset_info.html",
    "title": "Airbnb Dataset Description",
    "section": "",
    "text": "From the project website: http://insideairbnb.com/about/\nInside Airbnb is a mission driven project that provides data and advocacy about Airbnb’s impact on residential communities.\nThe dataset contains 7000 Amsterdam listings (each with 75 features) and 300k textual reviews from airbnb.com, scraped in March 2023. Oldest listings have reviews from as far as 2009. Additionally, for each listing, the dataset includes the price of stay for each day (night) since March 2023 till March 2024, as well as whether this day is available for booking, also scraped in March 2023.\nThe features descriptions can be found in this sheet from Inside Airbnb.\nFile descriptions: - listings.csv: Detailed Listings data - calendar.csv: Detailed Calendar Data - reviews.csv: Detailed Review Data - listings.csv: Summary information and metrics for listings in Amsterdam (good for visualisations). - reviews.csv: Summary Review data and Listing ID (to facilitate time based analytics and visualisations linked to a listing). - neighbourhoods.csv Neighbourhood list for geo filter. Sourced from city or open source GIS files. - neighbourhoods.geojson GeoJSON file of neighbourhoods of the city.\n\n\n\nExploratory Data Analysis\n\nGeomaps\n\nPrice and Rating Prediction\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfile = \"D:/Data Science/Inside AirBnB - Netherlands/Amsterdam/\"\n\n\n\n\n\n\nCode\nlistings = pd.read_csv(file + \"listings.csv\")\nlistings.head(2)\n\n\n\n\n\n\n\n\n\n\nid\nname\nhost_id\nhost_name\nneighbourhood_group\nneighbourhood\nlatitude\nlongitude\nroom_type\nprice\nminimum_nights\nnumber_of_reviews\nlast_review\nreviews_per_month\ncalculated_host_listings_count\navailability_365\nnumber_of_reviews_ltm\nlicense\n\n\n\n\n0\n2818\nQuiet Garden View Room & Super Fast Wi-Fi\n3159\nDaniel\nNaN\nOostelijk Havengebied - Indische Buurt\n52.36435\n4.94358\nPrivate room\n69\n3\n322\n2023-02-28\n1.90\n1\n44\n37\n0363 5F3A 5684 6750 D14D\n\n\n1\n20168\nStudio with private bathroom in the centre 1\n59484\nAlexander\nNaN\nCentrum-Oost\n52.36407\n4.89393\nPrivate room\n106\n1\n339\n2020-04-09\n2.14\n2\n0\n0\n0363 CBB3 2C10 0C2A 1E29\n\n\n\n\n\n\n\n\n\n\nCode\nlistings_detailed = pd.read_csv(file + \"listings_detailed.csv\")\nlistings_detailed.head(2)\n\n\n\n\n\n\n\n\n\n\nid\nlisting_url\nscrape_id\nlast_scraped\nsource\nname\ndescription\nneighborhood_overview\npicture_url\nhost_id\n...\nreview_scores_communication\nreview_scores_location\nreview_scores_value\nlicense\ninstant_bookable\ncalculated_host_listings_count\ncalculated_host_listings_count_entire_homes\ncalculated_host_listings_count_private_rooms\ncalculated_host_listings_count_shared_rooms\nreviews_per_month\n\n\n\n\n0\n2818\nhttps://www.airbnb.com/rooms/2818\n20230309202119\n2023-03-09\ncity scrape\nQuiet Garden View Room & Super Fast Wi-Fi\nQuiet Garden View Room & Super Fast Wi-Fi&lt;br /...\nIndische Buurt (\"Indies Neighborhood\") is a ne...\nhttps://a0.muscache.com/pictures/10272854/8dcc...\n3159\n...\n4.98\n4.69\n4.81\n0363 5F3A 5684 6750 D14D\nf\n1\n0\n1\n0\n1.90\n\n\n1\n311124\nhttps://www.airbnb.com/rooms/311124\n20230309202119\n2023-03-10\ncity scrape\n*historic centre* *bright* *canal view* *jordaan*\n&gt; Please be so kind to book ONLY AFTER conta...\nPerfect location in the lively centre. All his...\nhttps://a0.muscache.com/pictures/5208672/5bb60...\n1600010\n...\n4.92\n4.93\n4.60\n0363 59D8 7D30 6CFA DC81\nf\n1\n1\n0\n0\n0.66\n\n\n\n\n2 rows × 75 columns\n\n\n\n\n\n\n\n\n\nCode\ncalendar = pd.read_csv(file + 'calendar.csv')\ncalendar.head(4)\n\n\n\n\n\n\n\n\n\n\nlisting_id\ndate\navailable\nprice\nadjusted_price\nminimum_nights\nmaximum_nights\n\n\n\n\n0\n2818\n2023-03-09\nf\n$69.00\n$69.00\n3.0\n1125.0\n\n\n1\n2818\n2023-03-10\nf\n$69.00\n$69.00\n3.0\n1125.0\n\n\n2\n2818\n2023-03-11\nf\n$69.00\n$69.00\n3.0\n1125.0\n\n\n3\n2818\n2023-03-12\nf\n$69.00\n$69.00\n3.0\n1125.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nreviews = pd.read_csv(file + 'reviews.csv')\nreviews.head(4)\n\n\n\n\n\n\n\n\n\n\nlisting_id\ndate\n\n\n\n\n0\n2818\n2009-03-30\n\n\n1\n2818\n2009-04-24\n\n\n2\n2818\n2009-05-03\n\n\n3\n2818\n2009-05-18\n\n\n\n\n\n\n\n\n\n\nCode\nreviews_detailed = pd.read_csv(file + 'reviews_detailed.csv')\nreviews_detailed.head(4)\n\n\n\n\n\n\n\n\n\n\nlisting_id\nid\ndate\nreviewer_id\nreviewer_name\ncomments\n\n\n\n\n0\n2818\n1191\n2009-03-30\n10952\nLam\nDaniel is really cool. The place was nice and ...\n\n\n1\n2818\n1771\n2009-04-24\n12798\nAlice\nDaniel is the most amazing host! His place is ...\n\n\n2\n2818\n1989\n2009-05-03\n11869\nNatalja\nWe had such a great time in Amsterdam. Daniel ...\n\n\n3\n2818\n2797\n2009-05-18\n14064\nEnrique\nVery professional operation. Room is very clea...\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nneighbourhoods = pd.read_csv(file + 'neighbourhoods.csv')\nneighbourhoods.head(4)\n\n\n\n\n\n\n\n\n\n\nneighbourhood_group\nneighbourhood\n\n\n\n\n0\nNaN\nBijlmer-Centrum\n\n\n1\nNaN\nBijlmer-Oost\n\n\n2\nNaN\nBos en Lommer\n\n\n3\nNaN\nBuitenveldert - Zuidas\n\n\n\n\n\n\n\n\n\n\nCode\nfrom json.decoder import JSONDecoder\n\n# neighbourhoods_geojson = JSONDecoder().decode((file + 'neighbourhoods.geojson').read_text())",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "airbnb_dataset_info.html#motivation",
    "href": "airbnb_dataset_info.html#motivation",
    "title": "Airbnb Dataset Description",
    "section": "",
    "text": "Exploratory Data Analysis\n\nGeomaps\n\nPrice and Rating Prediction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "airbnb_dataset_info.html#data-import",
    "href": "airbnb_dataset_info.html#data-import",
    "title": "Airbnb Dataset Description",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfile = \"D:/Data Science/Inside AirBnB - Netherlands/Amsterdam/\"\n\n\n\n\n\n\nCode\nlistings = pd.read_csv(file + \"listings.csv\")\nlistings.head(2)\n\n\n\n\n\n\n\n\n\n\nid\nname\nhost_id\nhost_name\nneighbourhood_group\nneighbourhood\nlatitude\nlongitude\nroom_type\nprice\nminimum_nights\nnumber_of_reviews\nlast_review\nreviews_per_month\ncalculated_host_listings_count\navailability_365\nnumber_of_reviews_ltm\nlicense\n\n\n\n\n0\n2818\nQuiet Garden View Room & Super Fast Wi-Fi\n3159\nDaniel\nNaN\nOostelijk Havengebied - Indische Buurt\n52.36435\n4.94358\nPrivate room\n69\n3\n322\n2023-02-28\n1.90\n1\n44\n37\n0363 5F3A 5684 6750 D14D\n\n\n1\n20168\nStudio with private bathroom in the centre 1\n59484\nAlexander\nNaN\nCentrum-Oost\n52.36407\n4.89393\nPrivate room\n106\n1\n339\n2020-04-09\n2.14\n2\n0\n0\n0363 CBB3 2C10 0C2A 1E29\n\n\n\n\n\n\n\n\n\n\nCode\nlistings_detailed = pd.read_csv(file + \"listings_detailed.csv\")\nlistings_detailed.head(2)\n\n\n\n\n\n\n\n\n\n\nid\nlisting_url\nscrape_id\nlast_scraped\nsource\nname\ndescription\nneighborhood_overview\npicture_url\nhost_id\n...\nreview_scores_communication\nreview_scores_location\nreview_scores_value\nlicense\ninstant_bookable\ncalculated_host_listings_count\ncalculated_host_listings_count_entire_homes\ncalculated_host_listings_count_private_rooms\ncalculated_host_listings_count_shared_rooms\nreviews_per_month\n\n\n\n\n0\n2818\nhttps://www.airbnb.com/rooms/2818\n20230309202119\n2023-03-09\ncity scrape\nQuiet Garden View Room & Super Fast Wi-Fi\nQuiet Garden View Room & Super Fast Wi-Fi&lt;br /...\nIndische Buurt (\"Indies Neighborhood\") is a ne...\nhttps://a0.muscache.com/pictures/10272854/8dcc...\n3159\n...\n4.98\n4.69\n4.81\n0363 5F3A 5684 6750 D14D\nf\n1\n0\n1\n0\n1.90\n\n\n1\n311124\nhttps://www.airbnb.com/rooms/311124\n20230309202119\n2023-03-10\ncity scrape\n*historic centre* *bright* *canal view* *jordaan*\n&gt; Please be so kind to book ONLY AFTER conta...\nPerfect location in the lively centre. All his...\nhttps://a0.muscache.com/pictures/5208672/5bb60...\n1600010\n...\n4.92\n4.93\n4.60\n0363 59D8 7D30 6CFA DC81\nf\n1\n1\n0\n0\n0.66\n\n\n\n\n2 rows × 75 columns\n\n\n\n\n\n\n\n\n\nCode\ncalendar = pd.read_csv(file + 'calendar.csv')\ncalendar.head(4)\n\n\n\n\n\n\n\n\n\n\nlisting_id\ndate\navailable\nprice\nadjusted_price\nminimum_nights\nmaximum_nights\n\n\n\n\n0\n2818\n2023-03-09\nf\n$69.00\n$69.00\n3.0\n1125.0\n\n\n1\n2818\n2023-03-10\nf\n$69.00\n$69.00\n3.0\n1125.0\n\n\n2\n2818\n2023-03-11\nf\n$69.00\n$69.00\n3.0\n1125.0\n\n\n3\n2818\n2023-03-12\nf\n$69.00\n$69.00\n3.0\n1125.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nreviews = pd.read_csv(file + 'reviews.csv')\nreviews.head(4)\n\n\n\n\n\n\n\n\n\n\nlisting_id\ndate\n\n\n\n\n0\n2818\n2009-03-30\n\n\n1\n2818\n2009-04-24\n\n\n2\n2818\n2009-05-03\n\n\n3\n2818\n2009-05-18\n\n\n\n\n\n\n\n\n\n\nCode\nreviews_detailed = pd.read_csv(file + 'reviews_detailed.csv')\nreviews_detailed.head(4)\n\n\n\n\n\n\n\n\n\n\nlisting_id\nid\ndate\nreviewer_id\nreviewer_name\ncomments\n\n\n\n\n0\n2818\n1191\n2009-03-30\n10952\nLam\nDaniel is really cool. The place was nice and ...\n\n\n1\n2818\n1771\n2009-04-24\n12798\nAlice\nDaniel is the most amazing host! His place is ...\n\n\n2\n2818\n1989\n2009-05-03\n11869\nNatalja\nWe had such a great time in Amsterdam. Daniel ...\n\n\n3\n2818\n2797\n2009-05-18\n14064\nEnrique\nVery professional operation. Room is very clea...\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nneighbourhoods = pd.read_csv(file + 'neighbourhoods.csv')\nneighbourhoods.head(4)\n\n\n\n\n\n\n\n\n\n\nneighbourhood_group\nneighbourhood\n\n\n\n\n0\nNaN\nBijlmer-Centrum\n\n\n1\nNaN\nBijlmer-Oost\n\n\n2\nNaN\nBos en Lommer\n\n\n3\nNaN\nBuitenveldert - Zuidas\n\n\n\n\n\n\n\n\n\n\nCode\nfrom json.decoder import JSONDecoder\n\n# neighbourhoods_geojson = JSONDecoder().decode((file + 'neighbourhoods.geojson').read_text())",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "preprocessing.html",
    "href": "preprocessing.html",
    "title": "Pre-Processing Data",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C', '#28AFB0']\nfile = \"D:/Career/Data Science/Portfolios/Inside AirBnB - Netherlands/Amsterdam/\"\n\nlistings = pd.read_csv(file + \"listings.csv\")\nlistings_detailed = pd.read_csv(file + \"listings_detailed.csv\")\ncalendar = pd.read_csv(file + 'calendar.csv')\nreviews = pd.read_csv(file + 'reviews.csv')\nreviews_detailed = pd.read_csv(file + 'reviews_detailed.csv')\nneighbourhoods = pd.read_csv(file + 'neighbourhoods.csv')",
    "crumbs": [
      "Pre-Processing"
    ]
  },
  {
    "objectID": "preprocessing.html#missing-values",
    "href": "preprocessing.html#missing-values",
    "title": "Pre-Processing Data",
    "section": "1. Missing Values",
    "text": "1. Missing Values\nWe will start our project by dealing with potential missing values.\n\nListings dataset\nLet’s inspect the listings dataset first:\n\nlistings.shape\n\n(6998, 18)\n\n\nWe have that the original dataset contains 6,998 observations and 18 variables. Let’s check know the potential missing values:\n\n\nCode\nlistings.isna().sum() / len(listings) * 100\n\n\nid                                  0.000000\nname                                0.000000\nhost_id                             0.000000\nhost_name                           0.000000\nneighbourhood_group               100.000000\nneighbourhood                       0.000000\nlatitude                            0.000000\nlongitude                           0.000000\nroom_type                           0.000000\nprice                               0.000000\nminimum_nights                      0.000000\nnumber_of_reviews                   0.000000\nlast_review                         9.745642\nreviews_per_month                   9.745642\ncalculated_host_listings_count      0.000000\navailability_365                    0.000000\nnumber_of_reviews_ltm               0.000000\nlicense                             0.342955\ndtype: float64\n\n\nSince neighbourhood_group has only NA’s and license does not seem to be a relevant variable, we will drop them:\n\ndrop_var = ['neighbourhood_group', 'license']\nlistings = listings.drop(columns = drop_var)\nlistings.shape\n\n(6998, 16)\n\n\nWhat about last_review and reviews_per_month? No more than 9.7% of them are missing values. Seems reasonable to just drop those, instead of applying imputation methods:\n\nna_vars = ['last_review', 'reviews_per_month']\nlistings.dropna(subset=na_vars, axis=0, inplace=True)\nprint('Observations:', len(listings) )\n\nObservations: 6316\n\n\nThis lead us to 6,316 observations.\n\n\ncalendar dataset\nWe repeat the process with the calendar dataset:\n\n\nCode\nprint( calendar.shape )\nprint ( calendar.isna().sum() / len(calendar) * 100 )\n\n\n(2554278, 7)\nlisting_id        0.000000\ndate              0.000000\navailable         0.000000\nprice             0.029715\nadjusted_price    0.029715\nminimum_nights    0.000078\nmaximum_nights    0.000078\ndtype: float64\n\n\nSince the proportion of missing values is low, let’s just drop those observations:\n\ncalendar.dropna(axis=0, inplace=True)\nprint('Observations:', len(calendar) )\n\nObservations: 2553517\n\n\n\n\nreviews dataset\nSame process with the reviews dataset:\n\n\nCode\nprint( reviews_detailed.shape )\nprint( reviews_detailed.isna().sum() / len(reviews_detailed) * 100 )\n\n\n(339805, 6)\nlisting_id       0.000000\nid               0.000000\ndate             0.000000\nreviewer_id      0.000000\nreviewer_name    0.000000\ncomments         0.004414\ndtype: float64\n\n\n\n\nCode\nreviews_detailed.dropna(axis=0, inplace=True)\nprint('Observations:', len(reviews_detailed) )\n\n\nObservations: 339790",
    "crumbs": [
      "Pre-Processing"
    ]
  },
  {
    "objectID": "preprocessing.html#outliers",
    "href": "preprocessing.html#outliers",
    "title": "Pre-Processing Data",
    "section": "2. Outliers",
    "text": "2. Outliers\nNow we can check and remove potential outliers in our dataset. Let’s start by checking our price variable, which will be our main outcome of interest.\n\nlistings - Price\nPlotting an histogram on price already tell of something about having the existence of outliers:\n\n\nCode\nimport seaborn as sns\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\n\n# Set up Figure\n#fig, ax = plt.subplots(figsize=(8,4))\n\n# Hist + KDE\nsns.displot(data=listings, x=\"price\", kde=True, color=my_colors[0], aspect=8/5)\n\n# Labels\nplt.xlabel('Price')\nplt.ylabel('')\nplt.title('Distribution of Prices')\n\n# Show the Plot\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Distribution of Price\n\n\n\n\n\nFigure 1 reveals a pronounced tail towards the higher end, indicating the presence of outliers—entries with significantly higher prices compared to the majority of listings. These outliers can skew statistical analysis and machine learning models, potentially leading to inaccurate predictions. Employing the Interquartile Range (IQR) approach is a robust method for detecting and managing outliers.\nBy calculating the IQR, which represents the range between the 25th and 75th percentiles of the data distribution, we can identify values that fall beyond a certain multiple of the IQR from the quartiles. Let’s identify the outliers by calculating the IQR:\n\n# calculate IQR for column Height\nQ1 = listings['price'].quantile(0.25)\nQ3 = listings['price'].quantile(0.75)\nIQR = Q3 - Q1\n\n# identify outliers\nthres = 1.5\noutliers = listings[(listings['price'] &lt; Q1 - thres * IQR) | (listings['price'] &gt; Q3 + thres * IQR)]\n\n# drop rows containing outliers\nlistings = listings.drop(outliers.index)\n\nWith this we can plot again our histogram without the outliers:\n\n\nCode\nimport seaborn as sns\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\n\n# Set up Figure\n#fig, ax = plt.subplots(figsize=(8,4))\n\n# Hist + KDE\nsns.displot(data=listings, x=\"price\", kde=True, color=my_colors[0], aspect=8/5)\n\n# Labels\nplt.xlabel('Price')\nplt.ylabel('')\nplt.title('Distribution of Prices')\n\n# Show the Plot\nplt.show()\nprint(listings.shape)\n\n\n\n\n\n\n\n\nFigure 2: Distribution of Price (No Outliers)\n\n\n\n\n\n(6018, 16)\n\n\nThe histogram in Figure 2 now exhibits a more symmetrical and bell-shaped distribution. This indicates a more normalized spread of prices, suggesting that extreme outliers have been successfully mitigated.\nNotably, with outliers removed, the average price per night for Airbnb listings in Amsterdam is approximately 190 euros.",
    "crumbs": [
      "Pre-Processing"
    ]
  },
  {
    "objectID": "preprocessing.html#data-export",
    "href": "preprocessing.html#data-export",
    "title": "Pre-Processing Data",
    "section": "Data Export",
    "text": "Data Export\nWe’ll export this finalized dataset. Going forward, all our analyses will rely on this processed dataset, which has undergone outlier removal using the IQR method.\n\nlistings.to_csv('listings_processed.csv', index=False)\ncalendar.to_csv('calendar_processed.csv', index=False)\n# reviews_detailed.to_csv('reviews_processed.csv', index=False)",
    "crumbs": [
      "Pre-Processing"
    ]
  },
  {
    "objectID": "price_prediction_models.html",
    "href": "price_prediction_models.html",
    "title": "Geo-Spatial Analysis",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C', '#28AFB0']\n\nlistings = pd.read_csv(\"listings_processed.csv\") # processed data\nFinally, we delve into the realm of machine learning to develop a predictive model aimed at estimating listing prices for Airbnb accommodations in Amsterdam. Leveraging a dataset enriched with a variety of features including location, property type, and amenities, our objective is to build a robust model capable of accurately predicting listing prices.\nTo ensure the reliability and generalizability of our model, we will employ k-fold cross-validation with k=10, a widely-used technique for assessing model performance and mitigating overfitting.\nAdditionally, we will explore the effectiveness of four distinct machine learning algorithms: Lasso regression, Decision Tree, Bagging, and Random Forest. By evaluating the performance of these models against our dataset, we aim to identify the most suitable approach for predicting listing prices.",
    "crumbs": [
      "ML Price Prediction"
    ]
  },
  {
    "objectID": "price_prediction_models.html#pre-processing",
    "href": "price_prediction_models.html#pre-processing",
    "title": "Geo-Spatial Analysis",
    "section": "Pre-processing",
    "text": "Pre-processing\nIn the pre-processing phase, we strategically removed irrelevant variables, including listing names, IDs, host names, and other non-predictive attributes, streamlining the dataset to focus solely on features with direct relevance to price prediction:\n\n\nCode\nlistings = listings.drop(columns=[\"name\", \"host_id\", \"host_name\", \"last_review\"])\nlistings.shape\n\n\n(6018, 12)\n\n\n\nCategorical Data (dummies)\nAdditionally, recognizing the significance of categorical features in our analysis, we applied one-hot encoding to convert categorical variables into dummies, thereby facilitating their integration into our machine learning models.\nneighbourhood, room_type are not ordinal, so we will just create dummy variables with them:\n\nlistings_1 = pd.get_dummies(listings, columns=['neighbourhood'], drop_first=True)\nlistings_2 = pd.get_dummies(listings_1, columns=['room_type'], drop_first=True)\n\nlistings_2.shape\n\n(6018, 34)\n\n\n\n\nFeature & Target\nNow, we define our features and target variables. The features (X) represent the input data used to make predictions, while the target variable (y) represents the outcome we seek to predict:\n\n# Defining features and targets\nX = listings_2.drop(columns=[\"price\"])\ny = listings_2[\"price\"]\nprint(X.shape, y.shape)\n\n(6018, 33) (6018,)\n\n\nWe extract features by removing the “price” column from our dataset listings_2, ensuring that our model is trained solely on relevant attributes and does not inadvertently incorporate the target variable during training.\nSubsequently, we isolate the target variable by assigning the “price” column to the variable y, delineating the values that our model will endeavor to predict.\n\n\nData Normalization\nWe recognize the importance of data normalization —a process aimed at standardizing the scale of numerical features within our dataset. To achieve this, we utilize the MinMaxScaler from the sklearn.preprocessing module, which rescales each feature to a specified range, typically between 0 and 1.\nHere, we apply normalization data on: latitude, longitude, price, minimum_nights, number_of_reviews, reviews_per_month, calculated_hosts_listings_count, availability_365, number_of_reviews_ltm\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nnum_var = ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', \n           'calculated_hosts_listings_count', 'availability_365', 'number_of_reviews_ltm'] ## EXCEPT PRICE\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\n\n\nCode\npd.DataFrame(X).head(3)\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n\n\n0\n0.000000e+00\n0.549117\n0.693964\n0.002\n0.139021\n0.016939\n0.000000\n0.120548\n0.022602\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n2.074478e-14\n0.547040\n0.510564\n0.000\n0.146384\n0.019089\n0.047619\n0.000000\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n2\n2.997292e-14\n0.721695\n0.502992\n0.002\n0.106973\n0.016222\n0.000000\n0.038356\n0.012217\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n3 rows × 33 columns",
    "crumbs": [
      "ML Price Prediction"
    ]
  },
  {
    "objectID": "price_prediction_models.html#training-and-test-data",
    "href": "price_prediction_models.html#training-and-test-data",
    "title": "Geo-Spatial Analysis",
    "section": "Training and Test Data",
    "text": "Training and Test Data\nWe will use k-fold Cross Validation by running the cross_validate function. Here, we will implement cross validation with a Random Forest classifier and calculate the average test score across k=10 folds. This is how it works: - cv_results = cross_validate(MODEL, X, y, cv=10)",
    "crumbs": [
      "ML Price Prediction"
    ]
  },
  {
    "objectID": "price_prediction_models.html#a.-linear-regression",
    "href": "price_prediction_models.html#a.-linear-regression",
    "title": "Geo-Spatial Analysis",
    "section": "A. Linear Regression",
    "text": "A. Linear Regression\nWe start our modeling endeavors by implementing a Linear Regression approach.\nWe also compute the root mean squared error (RMSE) as a metric of predictive accuracy. Subsequently, we aggregate the RMSE scores obtained from each cross-validation fold to derive an average performance metric.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_validate\n\n# Initialize Linear Regression model\nlinear_reg = LinearRegression()\n\n# Cross validation and performance evaluation\nrmse_ols = cross_validate(linear_reg, X, y, cv=10, scoring='neg_root_mean_squared_error') \n\n# Average\nrmse_ols = pd.DataFrame(rmse_ols)\nprint( rmse_ols[\"test_score\"].mean() )\n\n-78.22445660007757",
    "crumbs": [
      "ML Price Prediction"
    ]
  },
  {
    "objectID": "price_prediction_models.html#b.-lasso-regression",
    "href": "price_prediction_models.html#b.-lasso-regression",
    "title": "Geo-Spatial Analysis",
    "section": "B. Lasso Regression",
    "text": "B. Lasso Regression\nWe also employ Lasso Regression —a technique renowned for its ability to perform variable selection and regularization, thereby enhancing model interpretability and generalization.\nPrior to model training, we embark on a hyperparameter tuning process to identify the optimal alpha value —a crucial parameter governing the strength of regularization in Lasso Regression.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LassoCV\n\n# Define a range of alpha values to search over\nalphas = [0.001, 0.01, 0.1, 1.0, 10.0]\n\n# Initialize LassoCV model with cross-validation\nlasso_cv = LassoCV(alphas=alphas, cv=5)\n\n# Fit the model\nlasso_cv.fit(X, y)\n\n# Get the MSE for each alpha value\nmse_values = np.mean(lasso_cv.mse_path_, axis=1)\n\n\n\nCode\n# Plot MSE against alpha\nplt.figure(figsize=(8, 6))\nplt.plot(lasso_cv.alphas_, mse_values)\nplt.axvline(x=0.01, color='grey', linestyle='--')\nplt.xscale('log')  # Set x-axis to log scale\nplt.xlabel('Alpha')\nplt.ylabel('Mean Squared Error (MSE)')\nplt.title('MSE vs. Alpha for Lasso Regression')\nplt.grid(False)\nplt.show()\n\nprint(\"Optimal Alpha:\", lasso_cv.alpha_)\n\n\n\n\n\n\n\n\nFigure 1: CV Lasso Hyperparameter Tuning\n\n\n\n\n\nOptimal Alpha: 0.01\n\n\nFrom Figure 1, we have identified the optimal hyperparameter for our Lasso Regression model to be alpha=0.01. With this optimal alpha value in hand, we are poised to proceed with training our Lasso Regression model:\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import cross_validate\n\nlasso = Lasso(alpha=lasso_cv.alpha_, random_state=42)\n\n# Cross validation and performance evaluation\nrmse_lasso = cross_validate(lasso, X, y, cv=10, scoring='neg_root_mean_squared_error') \n\n# Average\nrmse_lasso = pd.DataFrame(rmse_lasso)\nprint( rmse_lasso[\"test_score\"].mean() )\n\n-78.22394950116399",
    "crumbs": [
      "ML Price Prediction"
    ]
  },
  {
    "objectID": "price_prediction_models.html#c.-decision-tree",
    "href": "price_prediction_models.html#c.-decision-tree",
    "title": "Geo-Spatial Analysis",
    "section": "C. Decision Tree",
    "text": "C. Decision Tree\nFor Decision Trees, common hyperparameters include the maximum depth of the tree, minimum samples required to split an internal node, minimum samples required to be at a leaf node, and maximum features considered for splitting.\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Define hyperparameters for Decision Tree\nparam_grid_dt = {\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\n# Initialize Decision Tree Regressor\ndt_regressor = DecisionTreeRegressor(random_state=42)\n\n# Perform grid search cross-validation for Decision Tree\ngrid_search_dt = GridSearchCV(dt_regressor, param_grid_dt, cv=5, scoring='neg_mean_squared_error')\ngrid_search_dt.fit(X, y)\n\n# Get the best hyperparameters for Decision Tree\nbest_params_dt = grid_search_dt.best_params_\nprint( best_params_dt )\n\n{'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10}\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_validate\n\n# Initialize Decision Tree Regressor\ndecision_tree_reg = DecisionTreeRegressor(max_depth = best_params_dt['max_depth'],\n                                     min_samples_split = best_params_dt['min_samples_split'],\n                                     min_samples_leaf = best_params_dt['min_samples_leaf'],\n                                     max_features = best_params_dt['max_features'],\n                                     random_state = 42)\n\n# Cross validation and performance evaluation\nrmse_tree = cross_validate(decision_tree_reg, X, y, cv=10, scoring='neg_root_mean_squared_error') \n\n# Average\nrmse_tree = pd.DataFrame(rmse_tree)\nprint( rmse_tree[\"test_score\"].mean() )",
    "crumbs": [
      "ML Price Prediction"
    ]
  },
  {
    "objectID": "price_prediction_models.html#d.-bagging-ensemble-methods",
    "href": "price_prediction_models.html#d.-bagging-ensemble-methods",
    "title": "Geo-Spatial Analysis",
    "section": "D. Bagging (Ensemble Methods)",
    "text": "D. Bagging (Ensemble Methods)\nFor Bagging, additional hyperparameters such as the number of base estimators (trees), the number of features to consider, numbere fo samples to draw, etc.\nHowever, here we will use Random Search, due to computational limitations:\n\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\n\n# Define hyperparameters for Bagging\nparam_dist_bagging = {\n    'n_estimators': randint(10, 500),\n    'max_samples': [0.5, 0.75, 1.0],\n    'max_features': [0.5, 0.75, 1.0],\n    'bootstrap': [True, False],\n    'bootstrap_features': [True, False]\n}\n\n# Initialize base estimator (e.g., Decision Tree)\nbase_estimator = DecisionTreeRegressor(max_depth = best_params_dt['max_depth'],\n                                     min_samples_split = best_params_dt['min_samples_split'],\n                                     min_samples_leaf = best_params_dt['min_samples_leaf'],\n                                     max_features = best_params_dt['max_features'],\n                                     random_state = 42)\n\n# Initialize Bagging Regressor\nbagging_regressor = BaggingRegressor(base_estimator=base_estimator, random_state=42)\n\n\n# Perform randomized search cross-validation\nrandom_search_bagging = RandomizedSearchCV(bagging_regressor, param_distributions=param_dist_bagging, n_iter=8, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrandom_search_bagging.fit(X, y)\n\n# Get the best hyperparameters\nbest_params_bagging = random_search_bagging.best_params_\n\nprint( best_params_bagging )\n\n{'bootstrap': False, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.75, 'n_estimators': 455}\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.model_selection import cross_validate\n\n\n# Initialize Bagging Regressor with the best hyperparameters\nbagging_regressor = BaggingRegressor(base_estimator=base_estimator,\n                                     n_estimators=best_params_bagging['n_estimators'],\n                                     max_samples=best_params_bagging['max_samples'],\n                                     max_features=best_params_bagging['max_features'],\n                                     bootstrap=best_params_bagging['bootstrap'],\n                                     bootstrap_features=best_params_bagging['bootstrap_features'],\n                                     random_state=42)\n\n# Cross validation and performance evaluation\nrmse_bagging = cross_validate(bagging_reg, X, y, cv=10, scoring='neg_root_mean_squared_error') \n\n# Average\nrmse_bagging = pd.DataFrame(rmse_bagging)\nprint( rmse_bagging[\"test_score\"].mean() )",
    "crumbs": [
      "ML Price Prediction"
    ]
  },
  {
    "objectID": "price_prediction_models.html#e.-random-forest-ensemble-methods",
    "href": "price_prediction_models.html#e.-random-forest-ensemble-methods",
    "title": "Geo-Spatial Analysis",
    "section": "E. Random Forest (Ensemble Methods)",
    "text": "E. Random Forest (Ensemble Methods)\nFinally, we train our Random Forests model, where additional hyperparameters such as the number of trees in the forest and the number of features to consider for each split also need to be considered.\n\n# Define hyperparameters for Random Forest\nparam_dist_rf = {\n    'n_estimators': randint(100, 500),\n    'max_depth': [None] + list(range(10, 31, 5)),\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\n# Define the hyperparameter grid\n#param_dist_rf = {'n_estimators': [10, 50, 100], \n#              'max_features': [None,'sqrt', 'log2'], 'criterion': ['gini','entropy']}\n\n# Initialize Random Forest Regressor\nrf_regressor = RandomForestRegressor(random_state=42)\n\n# Perform grid search cross-validation for Random Forest\nrandom_search_rf = RandomizedSearchCV(rf_regressor, param_distributions=param_dist_rf, n_iter=8, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrandom_search_rf.fit(X, y)\n\n# Get the best hyperparameters for Random Forest\nbest_params_rf = random_search_rf.best_params_\nprint( best_params_rf )\n\n{'max_depth': 25, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 485}\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_validate\n\n# Initialize\nrf_regressor = RandomForestRegressor(n_estimators = best_params_rf['n_estimators'],\n                                      max_depth = best_params_rf['max_depth'],\n                                      min_samples_split = best_params_rf['min_samples_split'],\n                                      min_samples_leaf = best_params_rf['min_samples_leaf'],\n                                      max_features = best_params_rf['max_features'],\n                                      random_state = 42)\n\n# Cross validation and performance evaluation\nrmse_rf = cross_validate(rf_regressor, X, y, cv=10, scoring='neg_root_mean_squared_error') \n\n# Average\nrmse_rf = pd.DataFrame(rmse_rf)\nprint( rmse_rf[\"test_score\"].mean() )",
    "crumbs": [
      "ML Price Prediction"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C', '#28AFB0']\nfile = \"D:/Career/Data Science/Portfolios/Inside AirBnB - Netherlands/Amsterdam/\"\n\nlistings = pd.read_csv(\"listings_processed.csv\") # processed data\ncalendar = pd.read_csv('calendar_processed.csv') # processed data\nreviews_detailed = pd.read_csv('reviews_processed.csv') # processed data\nneighbourhoods = pd.read_csv(file + 'neighbourhoods.csv')\nIn this section, we will conduct an Exploratory Data Analysis (EDA) to gain a deeper understanding of the Amsterdam Airbnb dataset.\nThis EDA serves as a important step in extracting valuable information and identifying key factors that influence Airbnb pricing in Amsterdam.",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#price",
    "href": "eda.html#price",
    "title": "Exploratory Data Analysis",
    "section": "Price",
    "text": "Price\nAs previously shown, Figure 1 presents the distribution of price of all our units in the sample:\n\n\nCode\nimport seaborn as sns\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\n\n# Set up Figure\n#fig, ax = plt.subplots(figsize=(8,5))\n\n# Hist + KDE\nsns.displot(data=listings, x=\"price\", kde=True, color=my_colors[0], aspect=8/5)\n\n# Labels\nplt.xlabel('Price')\nplt.ylabel('')\nplt.title('Distribution of Price')\n\n# Show the Plot\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Distribution of Price\n\n\n\n\n\nThe average price per night for Airbnb listings in Amsterdam is approximately 190 euros.\nLet’s also check price evolution:\n\n\nCode\ncalendar.price = calendar.price.replace('[\\$,]', '', regex=True).astype(float)\n\n\n\n\nCode\nimport seaborn as sns\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\n\ncalendar['date'] = pd.to_datetime(calendar['date'])\nprice_series = calendar.groupby(\"date\")[\"price\"].agg([\"mean\",\"median\"]).reset_index()\n\n#Filter\nfilter_date = price_series['date'] &lt;= pd.Timestamp('2024-01-01')\n\n# Set up Figure\nfig, ax = plt.subplots(figsize=(8,4))\n\n# Line Plot\nsns.lineplot(price_series[filter_date], x='date', y='mean', color=my_colors[0])\n\n# Labels\nplt.xlabel('Date')\nplt.ylabel('Mean Price')\nplt.title('Avg. Price Trend')\n\n# Show the Plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Average Price Trend",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#property-characteristics---type",
    "href": "eda.html#property-characteristics---type",
    "title": "Exploratory Data Analysis",
    "section": "Property Characteristics - Type",
    "text": "Property Characteristics - Type\nWe compute summary statistics of prices categorized by room type, which include private room, entire home, hotel room, and shared room.\n\n\nCode\nlistings.groupby(\"room_type\")['price'].agg(['mean','count']).round(1)\n\n\n\n\nTable 1: Price by Type\n\n\n\n\n\n\n\n\n\n\n\nmean\ncount\n\n\nroom_type\n\n\n\n\n\n\nEntire home/apt\n214.4\n4294\n\n\nHotel room\n154.0\n58\n\n\nPrivate room\n140.9\n1630\n\n\nShared room\n95.9\n36\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\n\n# Set up Figure\nfig, ax = plt.subplots(figsize=(8,4))\n\n# Violin Plot\nsns.violinplot(data=listings, x=\"price\", y=\"room_type\", palette=my_colors)\n\n# Labels\nplt.xlabel('Price')\nplt.ylabel('')\nplt.title('Distribution of Prices, by Type of Room')\n\n# Show the Plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Distribution of Prices, by Type of Room\n\n\n\n\n\nUpon analyzing Table 1 and Figure 3, it’s evident that entire homes command the highest prices in the Amsterdam Airbnb market (214 euros), followed by hotel rooms (154 euros). This observation suggests that guests are willing to pay a premium for the privacy and amenities offered by entire accommodations.\nAdditionally, we note that private rooms exhibit less dispersion in prices compared to entire homes and hotel rooms. This might imply a more consistent pricing structure within the private room category, possibly influenced by factors such as location, amenities, and room size.\nWe also computed the average minimum nights required for booking across different room types in the Amsterdam Airbnb market:\n\n\nCode\nlistings.groupby(\"room_type\")['minimum_nights'].agg(['mean']).round(1)\n\n\n\n\nTable 2: Minimum Nights by Type\n\n\n\n\n\n\n\n\n\n\n\nmean\n\n\nroom_type\n\n\n\n\n\nEntire home/apt\n4.5\n\n\nHotel room\n1.3\n\n\nPrivate room\n3.3\n\n\nShared room\n1.5\n\n\n\n\n\n\n\n\n\n\n\nTable 2 suggest that entire homes and private rooms typically require a longer minimum stay compared to hotel rooms and shared rooms. One potential factor contributing to this discrepancy is economies of scale. Larger accommodations such as entire homes and private rooms may require longer minimum stays to offset operational costs and maximize profitability. Conversely, hotel rooms and shared rooms, which typically offer more compact and flexible accommodations, tend to have shorter minimum stay requirements.\n\n\nCode\nlistings.groupby(\"room_type\")['number_of_reviews'].agg(['mean','sum']).round(1).sort_values('mean', ascending= False)\n\n\n\n\nTable 3: Number of Reviews by Type\n\n\n\n\n\n\n\n\n\n\n\nmean\nsum\n\n\nroom_type\n\n\n\n\n\n\nHotel room\n138.9\n8610\n\n\nPrivate room\n120.9\n210680\n\n\nShared room\n115.7\n4514\n\n\nEntire home/apt\n22.5\n116001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlistings.groupby(\"room_type\")['reviews_per_month'].agg(['mean']).round(1).sort_values('mean', ascending= False)\n\n\n\n\nTable 4: Reviews per Month by Type\n\n\n\n\n\n\n\n\n\n\n\nmean\n\n\nroom_type\n\n\n\n\n\nShared room\n4.5\n\n\nHotel room\n2.7\n\n\nPrivate room\n2.4\n\n\nEntire home/apt\n0.6\n\n\n\n\n\n\n\n\n\n\n\nHotel rooms have the highest average number of reviews per listing, followed by private rooms and shared rooms, while entire homes/apartments have the lowest average number of reviews.\nThis suggests that hotel rooms tend to receive more feedback from guests compared to other accommodation types.",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#prices-by-neighbourhood",
    "href": "eda.html#prices-by-neighbourhood",
    "title": "Exploratory Data Analysis",
    "section": "Prices by Neighbourhood",
    "text": "Prices by Neighbourhood\n\n\nCode\nmy_statistics = ['mean','median', 'count']\n\nneigh_df = listings.groupby(\"neighbourhood\")['price'].agg(\n    my_statistics).round(1).sort_values(\n    \"mean\", ascending = False)\nneigh_df\n\n\n\n\nTable 5: Prices by Neighbourhood\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\ncount\n\n\nneighbourhood\n\n\n\n\n\n\n\nDe Pijp - Rivierenbuurt\n213.0\n200.0\n685\n\n\nCentrum-Oost\n210.2\n195.0\n584\n\n\nZuid\n205.6\n195.0\n368\n\n\nCentrum-West\n202.8\n180.0\n789\n\n\nDe Baarsjes - Oud-West\n200.8\n184.0\n979\n\n\nWesterpark\n193.7\n179.0\n429\n\n\nIJburg - Zeeburgereiland\n192.3\n170.0\n135\n\n\nOud-Oost\n190.3\n175.0\n366\n\n\nWatergraafsmeer\n185.3\n177.5\n172\n\n\nOud-Noord\n175.6\n158.5\n268\n\n\nBos en Lommer\n175.0\n160.0\n303\n\n\nBuitenveldert - Zuidas\n174.6\n166.0\n65\n\n\nGeuzenveld - Slotermeer\n173.2\n149.0\n81\n\n\nNoord-West\n170.9\n150.0\n158\n\n\nOostelijk Havengebied - Indische Buurt\n168.2\n150.0\n233\n\n\nNoord-Oost\n163.2\n146.0\n97\n\n\nOsdorp\n152.9\n122.5\n40\n\n\nSlotervaart\n152.8\n137.0\n126\n\n\nDe Aker - Nieuw Sloten\n141.8\n100.0\n40\n\n\nBijlmer-Centrum\n135.1\n100.0\n40\n\n\nBijlmer-Oost\n130.0\n130.0\n20\n\n\nGaasperdam - Driemond\n123.9\n100.0\n40\n\n\n\n\n\n\n\n\n\n\n\nIn our analysis of Amsterdam Airbnb listings, we’ve identified several neighborhoods commanding higher average prices, notably De Pijp - Rivierenbuurt, Centrum-Oost, Zuid, and Centrum-West. With average prices ranging from 202 to 213 euros per night, these neighborhoods emerge as premium destinations within the Amsterdam accommodation market. Notably, De Pijp - Rivierenbuurt, historically recognized for its affluent reputation, continues to attract visitors seeking upscale experiences.\n\n\nCode\n# Set up Figure\nfig, axs = plt.subplots(1, 2, figsize=(12,4))\n\n# Plot\nneigh_df.sort_values(\"count\")['count'].plot(ax=axs[0], color=my_colors[0], kind='barh')\nneigh_df.sort_values(\"mean\")['mean'].plot(ax=axs[1], color=my_colors[1], kind='barh')\n\n# Labels\naxs[0].set_title('Frequency')\naxs[1].set_title('Mean Price')\n\naxs[0].set_xlabel('')\naxs[1].set_xlabel('')\n\naxs[0].set_ylabel('neighbourhood')\naxs[1].set_ylabel('')\n\n# Show the Plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Count and Mean Price by Neighbourhood\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\n\n# Set up Figure\nfig, ax = plt.subplots(figsize=(8,4))\n\n# Filter\nsome_neighs = listings['neighbourhood'].isin(\n    ['Centrum-Oost','Zuid', 'De Pijp - Rivierenbuurt', 'Noord-Oost', 'Bijlmer-Centrum'])\n\n# Violin Plot\nsns.violinplot(data=listings[some_neighs], x=\"price\", y=\"neighbourhood\", palette=my_colors)\n\n# Labels\nplt.xlabel('Price')\nplt.ylabel('')\nplt.title('Distribution of Prices by Neighbourhood (subsample)')\n\n# Show the Plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Distribution of Prices by Neighbourhood (subsample)\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\n\ncalendar['date'] = pd.to_datetime(calendar['date'])\n\n#merge\nmerged_calendar = calendar.merge(listings[[\"id\",'neighbourhood']], left_on='listing_id', right_on='id', how=\"left\")\n\n# group\nprice_series = merged_calendar.groupby([\"date\", \"neighbourhood\"])[\"price\"].agg([\"mean\",\"median\"]).reset_index()\n\n#Filter\nfilter_date = price_series['date'] &lt;= pd.Timestamp('2024-01-01')\n\n# Filter\nsome_neighs = price_series['neighbourhood'].isin(\n    ['Centrum-Oost','Zuid', 'De Pijp - Rivierenbuurt', 'Noord-Oost', 'Bijlmer-Centrum'])\n\n\n## Set up Figure ##\n###################\n\nfig, ax = plt.subplots(figsize=(9,4))\n\n# Line Plot\nsns.lineplot(price_series[filter_date & some_neighs], x='date', y='mean', hue=\"neighbourhood\", palette=my_colors)\nsns.move_legend(ax, \"lower center\", bbox_to_anchor=(0.5, 1), ncol=3, title=None, frameon=False)\n\n\n# Labels\nplt.xlabel('Date')\nplt.ylabel('Mean Price')\nplt.title('')\n\n# Show the Plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Average Price Trend, by some Neighbourhoods\n\n\n\n\n\nAfter plotting the time series of average prices across the top neighborhoods and other neighborhoods in Amsterdam ( Figure 6 ), we observe a consistent and steady evolution of prices over time, with no significant increasing trend.\nThis stable pricing pattern suggests that the Amsterdam Airbnb market maintains a relatively balanced and predictable pricing environment across different neighborhoods. Despite fluctuations influenced by seasonal variations or occasional events, there is no clear upward trajectory in prices over the observed time period",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#reviews-per-month",
    "href": "eda.html#reviews-per-month",
    "title": "Exploratory Data Analysis",
    "section": "Reviews per Month",
    "text": "Reviews per Month\n\n\nCode\nimport seaborn as sns\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\n\n# Set up Figure\n#fig, ax = plt.subplots(figsize=(8,5))\n\n# Hist + KDE\nsns.jointplot(data=listings, y=\"reviews_per_month\", x=\"price\", hue='room_type', palette=my_colors)\n\n# Labels\nplt.ylabel('Reviews per month')\nplt.xlabel('Price')\n#plt.title('Distribution of Price')\n\n# Show the Plot\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8: Reviews per month and Price, by Type",
    "crumbs": [
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "geospatial_analysis.html",
    "href": "geospatial_analysis.html",
    "title": "Geo-Spatial Analysis",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C', '#28AFB0']\nfile = \"D:/Career/Data Science/Portfolios/Inside AirBnB - Netherlands/Amsterdam/\"\n\nlistings = pd.read_csv(\"listings_processed.csv\") # processed data\n#calendar = pd.read_csv('calendar_processed.csv') # processed data\n#neighbourhoods = pd.read_csv(file + 'neighbourhoods.csv')",
    "crumbs": [
      "Geospatial Analysis"
    ]
  },
  {
    "objectID": "geospatial_analysis.html#type",
    "href": "geospatial_analysis.html#type",
    "title": "Geo-Spatial Analysis",
    "section": "Type",
    "text": "Type\n\n\nCode\nimport contextily as ctx\n\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\n\ncustom_palette = {\n    'Entire home/apt': my_colors[0],\n    'Private room': my_colors[1],\n    'Shared room': my_colors[2],\n    'Hotel room': my_colors[3]\n}\n\n# Map column values to colors based on your custom palette\ncolors = gdf['room_type'].map(custom_palette)\n\n# Plot\nax = gdf.plot(column='room_type', alpha=0.7, color=colors, legend=True, figsize=(10, 10), markersize=8)\n# Add basemap from OpenStreetMap\nctx.add_basemap(ax, crs=gdf.crs.to_string(), zoom=13, source=ctx.providers.CartoDB.Positron)\n\n\n# Create a custom legend\nlegend_elements = [plt.Line2D([0], [0], marker='o', color=color, label=label, linestyle='None') \n                   for label, color in custom_palette.items()]\n\n# Place legend outside the plot area\nplt.legend(handles=legend_elements, loc='center', bbox_to_anchor=(0.5, -0.2), ncol=len(custom_palette))\n\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Listings by Room Type\n\n\n\n\n\n\n\nCode\nimport contextily as ctx\n\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\nroom_filter = gdf['room_type'].isin(['Shared room', 'Hotel room'])\n\ncustom_palette = {\n    'Shared room': my_colors[2],\n    'Hotel room': my_colors[3]\n}\n\n# Map column values to colors based on your custom palette\ncolors = gdf[room_filter]['room_type'].map(custom_palette)\n\n# Plot\nax = gdf[room_filter].plot(column='room_type', alpha=0.9, color=colors, legend=True, figsize=(10, 10))\n# Add basemap from OpenStreetMap\nctx.add_basemap(ax, crs=gdf.crs.to_string(), zoom=13, source=ctx.providers.CartoDB.Positron)\n\n\n# Create a custom legend\nlegend_elements = [plt.Line2D([0], [0], marker='o', color=color, label=label, linestyle='None') \n                   for label, color in custom_palette.items()]\n\n# Place legend outside the plot area\nplt.legend(handles=legend_elements, loc='center', bbox_to_anchor=(0.5, -0.2), ncol=len(custom_palette))\n\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Listings by Room Type (Filtered)",
    "crumbs": [
      "Geospatial Analysis"
    ]
  },
  {
    "objectID": "geospatial_analysis.html#price",
    "href": "geospatial_analysis.html#price",
    "title": "Geo-Spatial Analysis",
    "section": "Price",
    "text": "Price\nOne way to study price patterns with maps is to just plotting the location of each units by price gradient:\n\n\nCode\nimport contextily as ctx\n\n# Plot\nax = gdf.plot(column='price', cmap='plasma', alpha=0.6, legend=True, figsize=(10, 4), markersize=10)\n\n# OpenStreetMap\nctx.add_basemap(ax, crs=gdf.crs.to_string(), zoom=13, source=ctx.providers.CartoDB.Positron)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Listings by Price\n\n\n\n\n\nFigure 3 visualizes the spatial distribution of Airbnb listings in Amsterdam, with each unit plotted according to a gradient of prices. Darker shades represent higher-priced listings, while lighter shades indicate lower-priced ones.\nThe initial map encountered a challenge due to the high density of listings, resulting in significant overlap that obscured meaningful patterns. This overlap made it difficult to discern spatial trends and understand price variations.\nTo address this issue, we opted for a choropleth map illustrating the average price by neighborhood:\n\nimport pandas as pd\n\n# Read the GeoJSON file\namsterdam_geojson_file = \"neighbourhoods.geojson\"  # Replace with the path to your GeoJSON file\namsterdam_gdf = gpd.read_file(amsterdam_geojson_file)\n\n# Group by Neighbourhood\ndf_mean_p = pd.DataFrame(gdf.groupby('neighbourhood')['price'].agg('mean')).reset_index()\ngdf_neigh = pd.merge(amsterdam_gdf, df_mean_p, on='neighbourhood', how='inner')\n\n\n\nCode\nimport contextily as ctx\n\n# Visualize the data\nax = gdf_neigh.plot(column='price', alpha=0.8, legend=True,  figsize=(10, 4))\n\nctx.add_basemap(ax, crs=gdf_neigh.crs.to_string(),  zoom=13, source=ctx.providers.OpenStreetMap.Mapnik)\n\nplt.title('Choropleth Map of Amsterdam, by Price')\nplt.xlabel('')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Listings by Price (choro)\n\n\n\n\n\nThis approach allows for a clearer depiction of spatial disparities in pricing, enabling us to identify neighborhoods with higher or lower average prices. Figure 4 shows that highest prices are concentrated in central areas, particularly those near Amsterdam Central, reflecting the premium associated with proximity to major attractions and amenities. Conversely, lower prices tend to be observed in the south-eastern regions of Amsterdam, suggesting more affordable accommodations in these areas. Meanwhile, moderate prices are prevalent in the north-eastern neighborhoods, indicating a balance between accessibility and affordability.",
    "crumbs": [
      "Geospatial Analysis"
    ]
  },
  {
    "objectID": "geospatial_analysis.html#reviews",
    "href": "geospatial_analysis.html#reviews",
    "title": "Geo-Spatial Analysis",
    "section": "Reviews",
    "text": "Reviews\n\nimport pandas as pd\n\ndf_mean_r = pd.DataFrame(gdf.groupby('neighbourhood')['number_of_reviews'].agg('mean')).reset_index()\ngdf_neigh_r = pd.merge(amsterdam_gdf, df_mean_r, on='neighbourhood', how='inner')\n\n\n\nCode\nimport contextily as ctx\n\n# Visualize the data\nax = gdf_neigh_r.plot(column='number_of_reviews', alpha=0.8, legend=True,  figsize=(10, 4))\n\nctx.add_basemap(ax, crs=gdf_neigh_r.crs.to_string(),  zoom=13, source=ctx.providers.OpenStreetMap.Mapnik)\n\nplt.title('Choropleth Map of Amsterdam, by Avg. Number of Reviews')\nplt.xlabel('')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Listings by Avg. Number of Reviews\n\n\n\n\n\nIn our analysis focusing on the average number of reviews by neighborhood, we see in Figure 5 that Lutkemeer emerged as the neighborhood with the highest average number of reviews, indicating a notable level of guest activity and engagement within this area. Following closely, central neighborhoods also demonstrated a significant presence in terms of average number of reviews, reaffirming their popularity and desirability among guests visiting Amsterdam.",
    "crumbs": [
      "Geospatial Analysis"
    ]
  },
  {
    "objectID": "geospatial_analysis.html#room-type",
    "href": "geospatial_analysis.html#room-type",
    "title": "Geo-Spatial Analysis",
    "section": "Room Type",
    "text": "Room Type\n\n\nCode\nimport contextily as ctx\n\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\n\ncustom_palette = {\n    'Entire home/apt': my_colors[0],\n    'Private room': my_colors[1],\n    'Shared room': my_colors[2],\n    'Hotel room': my_colors[3]\n}\n\n# Map column values to colors based on your custom palette\ncolors = gdf['room_type'].map(custom_palette)\n\n# Plot\nax = gdf.plot(column='room_type', alpha=0.7, color=colors, legend=True, figsize=(10, 10), markersize=8)\n# Add basemap from OpenStreetMap\nctx.add_basemap(ax, crs=gdf.crs.to_string(), zoom=13, source=ctx.providers.CartoDB.Positron)\n\n\n# Create a custom legend\nlegend_elements = [plt.Line2D([0], [0], marker='o', color=color, label=label, linestyle='None') \n                   for label, color in custom_palette.items()]\n\n# Place legend outside the plot area\nplt.legend(handles=legend_elements, loc='center', bbox_to_anchor=(0.5, -0.2), ncol=len(custom_palette))\n\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Listings by Room Type\n\n\n\n\n\nIt’s clear from Figure 1 that most of the units are either entire homes or private rooms, whose spatial distribution is more or less similar similar. Let’s focus now only on the remaining units (shared, hotels):\n\n\nCode\nimport contextily as ctx\n\nmy_colors =['#28AFB0', '#F46036', '#F1E3D3', '#2D1E2F', '#26547C']\nroom_filter = gdf['room_type'].isin(['Shared room', 'Hotel room'])\n\ncustom_palette = {\n    'Shared room': my_colors[2],\n    'Hotel room': my_colors[3]\n}\n\n# Map column values to colors based on your custom palette\ncolors = gdf[room_filter]['room_type'].map(custom_palette)\n\n# Plot\nax = gdf[room_filter].plot(column='room_type', alpha=0.9, color=colors, legend=True, figsize=(10, 10))\n# Add basemap from OpenStreetMap\nctx.add_basemap(ax, crs=gdf.crs.to_string(), zoom=13, source=ctx.providers.CartoDB.Positron)\n\n\n# Create a custom legend\nlegend_elements = [plt.Line2D([0], [0], marker='o', color=color, label=label, linestyle='None') \n                   for label, color in custom_palette.items()]\n\n# Place legend outside the plot area\nplt.legend(handles=legend_elements, loc='center', bbox_to_anchor=(0.5, -0.2), ncol=len(custom_palette))\n\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Listings by Room Type (Filtered)",
    "crumbs": [
      "Geospatial Analysis"
    ]
  }
]